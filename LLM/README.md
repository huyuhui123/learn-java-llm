# 前言
我之前因为对ai感兴趣，所以也有在网上找书看，比如《python深度学习》、《LLMBook》，也有看一点《刘知远团队大模型公开课》的视频，
但在学完一些基础后，去知乎上看了下才发现大家已经讨论到搭建LLM了，也知道了LLM和以前模型的区别就在于参数规模上，而一般人是承担不了从零生成一个LLM大模型的成本的，而且开源的中文模型相对较少且不效果不高，并且我本地也运行不了LLM，
尝试如此困难，所以逐渐心灰意冷，也发现好像现在讨论的都是在有限资源下能做的努力，于是进入观望 ，而且现在ai在商业上虽然有趋势，但是还不明显

一句话来说就是，chatGPT好棒，能集成到java程序里或特定领域里肯定能提升效率，但是别说构建一个，我连本地运行一个的资格都不够，所以导致我是否犹豫，本来想着是顺着时代的潮流学习新知识的，但现在难以判断这件事我是否能做到。

> GPT-2：可以在单张RTX 2080（8GB显存）上运行，但性能会有所限制。
> 
> GPT-3：需要多张RTX 3090（每张24GB显存）或者一张A100（40GB显存）。

但是我觉得为了应对以后的ai趋势，现在还是应该做到要能说出ai的基础知识以及具备一定的动手能力
- 整理以前学习的ai基础、知道自己现在大致学到哪个地方了