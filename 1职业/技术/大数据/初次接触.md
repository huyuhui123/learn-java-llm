我个人来看，大数据本质就是为了解决数据量过大导致的一系列问题的解决方案，一般来说更用在统计业务数据、日志这种有大量数据的场景上

具体问题有：存储、计算、资源调度，他们下面都有很多解决方案，而hadoop就像是大数据的代理，这些问题都会包含在hadoop里面解决

下面就是我初步看到的架构，我觉得搭建好后最常用的就是用hive sql查数据，其他都是部署、管理数据的工作

和后端对比感觉没有冲突，我觉得之后可以搭建一个项目加深印象，再往深就只能看源码和特定领域下的使用了

```
hadoop 应用和大数据之间的中间层，感觉像大数据的代理层
    存储
        hdfs
    计算
        输入
            hive：sql转mapreduce
            flink：实时计算
        底层
            mapreduce： map、reduce 两个操作 读取分片数据做处理
            spark：使用内存加快计算速度的mapreduce

    查询
        hbase：大数据数据库
    计算资源管理
        yarn 资源申请、协调调度
```